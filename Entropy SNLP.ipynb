{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554 tagged words read\n",
      "926761 words read\n"
     ]
    }
   ],
   "source": [
    "# importing all the functions and \n",
    "\n",
    "import re\n",
    "from string import *\n",
    "import sys\n",
    "from nltk import *\n",
    "import math\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def importingBrownCorpusFromNLTK(outF):\n",
    "    \"importing tagged brown corpus from NLTK and writing on a file OutF\"\n",
    "    outF = open(outF,'w')\n",
    "    from nltk.corpus import brown\n",
    "    brown_news_tagged = brown.tagged_words(categories='news',simplify_tags=True)\n",
    "    print ('size', len(brown_news_tagged))\n",
    "    for i in brown_news_tagged:\n",
    "        outF.write(i[0]+'\\t'+i[1]+'\\n')\n",
    "    outF.close()\n",
    "\n",
    "def getWordsFromFile(inF):\n",
    "    \"get a list of words from a text file\"\n",
    "    lines = map(lambda x:x.replace('\\n','').lower(),open(inF).readlines())\n",
    "    words=[]\n",
    "    for line in lines:\n",
    "        for word in line.split():\n",
    "            words.append(word)\n",
    "    print (len(words),'words read')\n",
    "    return words\n",
    "\n",
    "def getTaggedWordsFromFile(inF):\n",
    "    \"get a list of pairs <word,POS> from a text file\"\n",
    "    lines = map(lambda x:x.replace('\\n','').lower(),open(inF).readlines())\n",
    "    words=[]\n",
    "    for line in lines:\n",
    "        word,pos = line.split('\\t')\n",
    "        words.append((word,pos))\n",
    "    print (len(words),'tagged words read')\n",
    "    return words\n",
    "\n",
    "def getTagsFromTaggedWords(l):\n",
    "    \"from a list of tagged words build a list of tags\"\n",
    "    return map(lambda x:x[1],l)\n",
    "\n",
    "def countNgrams(l,inic,end=0):\n",
    "    \"\"\"\n",
    "    From a list l (of words or pos), an inic position and an end position\n",
    "    a tuple(U,B,T) of dics corresponding to unigrams, bigrams and trigrams are built\n",
    "    \"\"\"\n",
    "    if end == 0:\n",
    "        end = len(l)\n",
    "    U={}\n",
    "    B={}\n",
    "    T={}\n",
    "    U[(l[inic])]=1\n",
    "    if (l[inic+1]) not in U:\n",
    "        U[(l[inic+1])]=1\n",
    "    else:\n",
    "        U[(l[inic+1])]+=1\n",
    "    B[(l[inic],l[inic+1])]=1\n",
    "    for i in range(inic+2,end):\n",
    "        if (l[i]) not in U:\n",
    "            U[(l[i])]=1\n",
    "        else:\n",
    "            U[(l[i])]+=1\n",
    "        if (l[i-1],l[i]) not in B:\n",
    "            B[(l[i-1],l[i])] = 1\n",
    "        else:\n",
    "            B[(l[i-1],l[i])] +=1\n",
    "        if (l[i-2],l[i-1],l[i]) not in T:\n",
    "            T[(l[i-2],l[i-1],l[i])] = 1\n",
    "        else:\n",
    "            T[(l[i-2],l[i-1],l[i])] +=1\n",
    "    return (U,B,T)\n",
    "\n",
    "\n",
    "# reading the corpus\n",
    "\n",
    "taggedWords = getTaggedWordsFromFile(\"corpus/taggedBrown.txt\")\n",
    "enWords = getWordsFromFile(\"corpus/en.txt\")\n",
    "\n",
    "\n",
    "ngrams_en = countNgrams(enWords,0,0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for calculating the entropy of unigram, bigram and trigram model of the corpus\n",
    "\n",
    "def getH(corpus):\n",
    "    en_prob = list(corpus[0].values())\n",
    "    sums = sum(en_prob)\n",
    "    en_prob = [x / sums for x in en_prob] \n",
    "    plogp_en = [x*math.log(x,2) for x in en_prob] \n",
    "    h_1 = -sum(plogp_en) \n",
    "    \n",
    "    keys_uni = corpus[0].keys()\n",
    "    dict_uni = dict(zip(keys_uni, en_prob))\n",
    "\n",
    "    dict_bi = {}\n",
    "    U = corpus[0]\n",
    "    B = corpus[1]\n",
    "    T = corpus[2]\n",
    "    for k in B.keys():\n",
    "        x = k[0]\n",
    "        dict_bi[k] = B[k] * 1.0/U[x] \n",
    "        h_2 = 0.0\n",
    "    for key in dict_bi.keys():\n",
    "        x = key[0]\n",
    "        y = key[1]\n",
    "        p_yx = dict_bi[key]\n",
    "        p_x = dict_uni[x]\n",
    "        h_2 -= p_x * p_yx * math.log(p_yx, 2.0)\n",
    "    \n",
    "    dict_tri = {}\n",
    "    for k in T.keys():\n",
    "        x = k[0]\n",
    "        y = k[1]  \n",
    "        dict_tri[k] = T[k] * 1.0/ B[(x,y)]\n",
    "    h_3 = 0.0\n",
    "    for key, value in T.items():\n",
    "        x = key[0]\n",
    "        y = key[1]\n",
    "        z = key[2]\n",
    "        p_x = dict_uni[x]\n",
    "        p_yx = dict_bi[(x,y)]\n",
    "        p_zxy = dict_tri[key]\n",
    "        h_3 -= p_x * p_yx * p_zxy * math.log(p_zxy, 2.0)\n",
    "    return (h_1, h_2, h_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy for unigram, bigram and trigram models of the english corpus\n",
      "\n",
      "H unigram (en) =  11.239725280044874 \n",
      "H bigram (en) =  5.927572453205569 \n",
      "H trigram (en) =  2.001937475160288\n"
     ]
    }
   ],
   "source": [
    "# entropy for unigram, bigram and trigram models of the english corpus\n",
    "\n",
    "H_uni, H_bi, H_tri  = getH(ngrams_en)\n",
    "\n",
    "\n",
    "print(\"\\nEntropy for unigram, bigram and trigram models of the english corpus\\n\\nH unigram (en) = \", \n",
    "      H_uni, \"\\nH bigram (en) = \", H_bi,\"\\nH trigram (en) = \", H_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting Ngrams from words in Brown Corpus\n",
    "\n",
    "def ngramsForBrown(text):\n",
    "    words_Brown = []\n",
    "    i = 1\n",
    "    for k in text:\n",
    "        x = k[0]\n",
    "        words_Brown.append(x) \n",
    "        i += 1\n",
    "\n",
    "    ngrams_Brown = countNgrams(words_Brown,0,0)\n",
    "    return(ngrams_Brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100554 50277 25138\n"
     ]
    }
   ],
   "source": [
    "# split into 3 corpora and calculate Ngrams\n",
    "\n",
    "full = taggedWords\n",
    "half = taggedWords[0:int(len(full)/2)]\n",
    "quarter = taggedWords[0:int(len(full)/4)]\n",
    "\n",
    "fullNgram = ngramsForBrown(full)\n",
    "halfNgram = ngramsForBrown(half)\n",
    "quarterNgram = ngramsForBrown(quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No smoothing\n",
      "\n",
      "H 100% (Brown) =  1.417687182050793\n",
      "Perplexity 100% (Brown) =  2.671568820700115\n",
      "\n",
      "H 50% (Brown) =  1.163228850120598\n",
      "Perplexity 50% (Brown) =  2.2395810067372444\n",
      "\n",
      "H 25% (Brown) =  0.972756645897304\n",
      "Perplexity 25% (Brown) =  1.962587050188545\n"
     ]
    }
   ],
   "source": [
    "# applying the entropy calculating function to 3 corpora\n",
    "\n",
    "print(\"\\n********************\\nNo smoothing\\n\")\n",
    "\n",
    "H_uni_full, H_bi_full, H_tri_full = getH(fullNgram)\n",
    "\n",
    "H_uni_half, H_bi_half, H_tri_half = getH(halfNgram)\n",
    "\n",
    "H_uni_quarter, H_bi_quarter, H_tri_quarter = getH(quarterNgram)\n",
    "\n",
    "print(\"H 100% (Brown) = \", H_tri_full)\n",
    "p_full = math.pow(2.0, H_tri_full)\n",
    "print(\"Perplexity 100% (Brown) = \", p_full)\n",
    "\n",
    "print(\"\\nH 50% (Brown) = \", H_tri_half)\n",
    "p_half = math.pow(2.0, H_tri_half)\n",
    "print(\"Perplexity 50% (Brown) = \", p_half)\n",
    "\n",
    "print(\"\\nH 25% (Brown) = \", H_tri_quarter)\n",
    "p_quarter = math.pow(2.0, H_tri_quarter)\n",
    "print(\"Perplexity 25% (Brown) = \", p_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting a dictionary of tags\n",
    "\n",
    "keys = []\n",
    "val = []\n",
    "i = 0\n",
    "for n in taggedWords:\n",
    "    keys.append(n[0])\n",
    "    val.append(n[1])\n",
    "dictTagged = dict(zip(keys, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# getting frequencies for bigrams and trigrams with smoothing 1 (x')\n",
    "\n",
    "def dict_bi_tri_xprime(corpus):\n",
    "    dict_bi_xprime = {}\n",
    "    for k in corpus[1].keys():\n",
    "        x = k[0]\n",
    "        y = k[1]\n",
    "        for k1 in dictTagged.keys():\n",
    "            if (x==k1):\n",
    "                key = (dictTagged[k1], y)\n",
    "                if key not in dict_bi_xprime.keys():\n",
    "                    dict_bi_xprime[key] = corpus[1][k]\n",
    "                else:\n",
    "                    dict_bi_xprime[key] += corpus[1][k]\n",
    "\n",
    "    dict_tri_xprime = {}\n",
    "    for k in corpus[2].keys():\n",
    "        x = k[0]\n",
    "        y = k[1]\n",
    "        z = k[2]\n",
    "        for k1 in dictTagged.keys():\n",
    "            if (x==k1):\n",
    "                key = (dictTagged[k1], y, z)\n",
    "                if key not in dict_tri_xprime.keys():\n",
    "                    dict_tri_xprime[key] = corpus[2][k]\n",
    "                else:\n",
    "                    dict_tri_xprime[key] += corpus[2][k]\n",
    "                \n",
    "    return(dict_bi_xprime, dict_tri_xprime)\n",
    "\n",
    "\n",
    "# getting frequencies for bigrams and trigrams with smoothing 2 (x', y')\n",
    "\n",
    "def dict_bi_tri_x_y_prime(corpus):\n",
    "    dict_bi_x_y_prime = {}\n",
    "    if (corpus == \"full\"):\n",
    "        dict_bi = dict_bi_xprime_full\n",
    "        dict_tri = dict_tri_xprime_full\n",
    "    elif (corpus == \"half\"):\n",
    "        dict_bi = dict_bi_xprime_half\n",
    "        dict_tri = dict_tri_xprime_half\n",
    "    else:\n",
    "        dict_bi = dict_bi_xprime_quarter\n",
    "        dict_tri = dict_tri_xprime_quarter\n",
    "\n",
    "    for k in dict_bi.keys():\n",
    "        x = k[0]\n",
    "        y = k[1]\n",
    "        for k1 in dictTagged.keys():\n",
    "            if (y==k1):\n",
    "                key = (x, dictTagged[k1])\n",
    "                if key not in dict_bi_x_y_prime.keys():\n",
    "                    dict_bi_x_y_prime[key] = dict_bi[k]\n",
    "                else:\n",
    "                    dict_bi_x_y_prime[key] += dict_bi[k]\n",
    "                    \n",
    "    dict_tri_x_y_prime = {}\n",
    "    for k in dict_tri.keys():\n",
    "        x = k[0]\n",
    "        y = k[1]\n",
    "        z = k[2]\n",
    "        for k1 in dictTagged.keys():\n",
    "            if (y==k1):\n",
    "                key = (x, dictTagged[k1], z)\n",
    "                if key not in dict_tri_x_y_prime.keys():\n",
    "                    dict_tri_x_y_prime[key] = dict_tri[k]\n",
    "                else:\n",
    "                    dict_tri_x_y_prime[key] += dict_tri[k]\n",
    "                \n",
    "    return(dict_bi_x_y_prime, dict_tri_x_y_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting frequencies for bigrams and trigrams with smoothing 1 (x') for 3 corpora\n",
    "\n",
    "dict_bi_xprime_full, dict_tri_xprime_full = dict_bi_tri_xprime(fullNgram)\n",
    "dict_bi_xprime_half, dict_tri_xprime_half = dict_bi_tri_xprime(halfNgram)\n",
    "dict_bi_xprime_quarter, dict_tri_xprime_quarter = dict_bi_tri_xprime(quarterNgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting ngram lists for smoothing 1 (x') for all corpora: corpus[0] is unigram frequencies,\n",
    "# corpus[1] is bigram frequencies, corpus[2] is trigram frequencies\n",
    "\n",
    "full_corpus = (FreqDist(list(getTagsFromTaggedWords(full))), dict_bi_xprime_full, dict_tri_xprime_full)\n",
    "half_corpus = (FreqDist(list(getTagsFromTaggedWords(half))), dict_bi_xprime_half, dict_tri_xprime_half)\n",
    "quarter_corpus = (FreqDist(list(getTagsFromTaggedWords(quarter))), dict_bi_xprime_quarter, dict_tri_xprime_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing x_prime\n",
      "\n",
      "H 100% (Brown) =  1.4447440743014064\n",
      "Perplexity 100% (Brown) =  2.722145297442119\n",
      "\n",
      "H 50% (Brown) =  2.889488148602813\n",
      "Perplexity 50% (Brown) =  7.410075020386244\n",
      "\n",
      "H 25% (Brown) =  5.779091242234998\n",
      "Perplexity 25% (Brown) =  54.91358680886062\n"
     ]
    }
   ],
   "source": [
    "# applying the entropy calculating function to 3 corpora\n",
    "\n",
    "print(\"\\n********************\\nSmoothing x_prime\\n\")\n",
    "\n",
    "H_uni_full, H_bi_full, H_tri_full = getH(full_corpus)\n",
    "\n",
    "H_uni_half, H_bi_half, H_tri_half = getH(half_corpus)\n",
    "\n",
    "H_uni_quarter, H_bi_quarter, H_tri_quarter = getH(quarter_corpus)\n",
    "\n",
    "print(\"H 100% (Brown) = \", H_tri_full)\n",
    "p_full = math.pow(2.0, H_tri_full)\n",
    "print(\"Perplexity 100% (Brown) = \", p_full)\n",
    "\n",
    "print(\"\\nH 50% (Brown) = \", H_tri_half)\n",
    "p_half = math.pow(2.0, H_tri_half)\n",
    "print(\"Perplexity 50% (Brown) = \", p_half)\n",
    "\n",
    "print(\"\\nH 25% (Brown) = \", H_tri_quarter)\n",
    "p_quarter = math.pow(2.0, H_tri_quarter)\n",
    "print(\"Perplexity 25% (Brown) = \", p_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting frequencies for bigrams and trigrams with smoothing 2 (x', y') for 3 corpora\n",
    "\n",
    "dict_bi_x_y_prime_full, dict_tri_x_y_prime_full = dict_bi_tri_x_y_prime(fullNgram)\n",
    "dict_bi_x_y_prime_half, dict_tri_x_y_prime_half = dict_bi_tri_x_y_prime(halfNgram)\n",
    "dict_bi_x_y_prime_quarter, dict_tri_x_y_prime_quarter = dict_bi_tri_x_y_prime(quarterNgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting ngram lists for smoothing 2 (x', y') for all corpora: corpus[0] is unigram frequencies,\n",
    "# corpus[1] is bigram frequencies, corpus[2] is trigram frequencies\n",
    "\n",
    "full_corpus = (FreqDist(list(getTagsFromTaggedWords(full))), dict_bi_x_y_prime_full, dict_tri_x_y_prime_full)\n",
    "half_corpus = (FreqDist(list(getTagsFromTaggedWords(half))), dict_bi_x_y_prime_half, dict_tri_x_y_prime_half)\n",
    "quarter_corpus = (FreqDist(list(getTagsFromTaggedWords(quarter))), dict_bi_x_y_prime_quarter, dict_tri_x_y_prime_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing x_prime, y_prime\n",
      "\n",
      "H 100% (Brown) =  1.4447440743014064\n",
      "Perplexity 100% (Brown) =  2.722145297442119\n",
      "\n",
      "H 50% (Brown) =  2.889488148602813\n",
      "Perplexity 50% (Brown) =  7.410075020386244\n",
      "\n",
      "H 25% (Brown) =  5.779091242234998\n",
      "Perplexity 25% (Brown) =  54.91358680886062\n"
     ]
    }
   ],
   "source": [
    "# applying the entropy calculating function to 3 corpora\n",
    "\n",
    "print(\"\\n********************\\nSmoothing x_prime, y_prime\\n\")\n",
    "\n",
    "H_uni_full, H_bi_full, H_tri_full = getH(full_corpus)\n",
    "\n",
    "H_uni_half, H_bi_half, H_tri_half = getH(half_corpus)\n",
    "\n",
    "H_uni_quarter, H_bi_quarter, H_tri_quarter = getH(quarter_corpus)\n",
    "\n",
    "print(\"H 100% (Brown) = \", H_tri_full)\n",
    "p_full = math.pow(2.0, H_tri_full)\n",
    "print(\"Perplexity 100% (Brown) = \", p_full)\n",
    "\n",
    "print(\"\\nH 50% (Brown) = \", H_tri_half)\n",
    "p_half = math.pow(2.0, H_tri_half)\n",
    "print(\"Perplexity 50% (Brown) = \", p_half)\n",
    "\n",
    "print(\"\\nH 25% (Brown) = \", H_tri_quarter)\n",
    "p_quarter = math.pow(2.0, H_tri_quarter)\n",
    "print(\"Perplexity 25% (Brown) = \", p_quarter)\n",
    "\n",
    "print(\"\\n\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
